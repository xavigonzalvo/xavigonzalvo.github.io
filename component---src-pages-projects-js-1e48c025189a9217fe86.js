"use strict";(self.webpackChunkxavi_s_page=self.webpackChunkxavi_s_page||[]).push([[853],{2723:function(e,t,a){a.r(t),a.d(t,{default:function(){return c}});var n=a(7294),i=a(3374),r=a(6947),s=a(4865);var l=function(e){let{title:t,summary:a,papers:i}=e;return n.createElement("div",{class:"group relative flex flex-col p-6 bg-white dark:bg-zinc-800/50 rounded-2xl border border-zinc-100 dark:border-zinc-700/40 shadow-sm hover:shadow-md transition-shadow"},n.createElement("h3",{class:"text-xl font-semibold text-zinc-800 dark:text-zinc-100 mb-3"},t),n.createElement("p",{class:"text-sm text-zinc-600 dark:text-zinc-400 mb-4"},a),n.createElement("div",{class:"space-y-3"},i.map(((e,t)=>n.createElement("div",{key:t,class:"flex items-start gap-3"},n.createElement("div",{class:"flex-shrink-0 mt-1"},n.createElement("svg",{class:"h-5 w-5 text-teal-500",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor"},n.createElement("path",{"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"}))),n.createElement("div",{class:"flex-1 min-w-0"},e.url?n.createElement("a",{href:e.url,class:"text-sm font-medium text-zinc-800 dark:text-zinc-100 hover:text-teal-500 dark:hover:text-teal-400 transition-colors",target:"_blank",rel:"noopener noreferrer"},e.title):n.createElement("span",{class:"text-sm font-medium text-zinc-800 dark:text-zinc-100"},e.title),n.createElement("p",{class:"text-xs text-zinc-500 dark:text-zinc-400 mt-1"},e.description)))))))};var o=function(){const e={title:"TTS",summary:"Developed high-quality text-to-speech synthesis systems, focusing on HMM-based and unit selection methods for real-time speech generation.",papers:[{title:"Recent Advances in Google Real-time HMM-driven Unit Selection Synthesizer",description:"Real-time speech synthesis combining HMM and unit selection for production-quality TTS.",url:"https://www.researchgate.net/publication/304335754_Recent_Advances_in_Google_Real-time_HMM-driven_Unit_Selection_Synthesizer"},{title:"Towards high-quality next-generation text-to-speech synthesis: A multidomain approach by automatic domain classification",description:"Multi-domain TTS approach using automatic domain classification for improved synthesis quality.",url:null}]},t={title:"AutoML",summary:"Pioneered adaptive neural architecture learning through ensemble methods and automated machine learning frameworks.",papers:[{title:"AdaNet: Adaptive Structural Learning of Artificial Neural Networks",description:"Framework for automatically learning neural network architectures through adaptive ensemble learning.",url:"http://proceedings.mlr.press/v70/cortes17a.html"},{title:"AdaNet: A Scalable and Flexible Framework for Automatically Learning Ensembles",description:"Scalable implementation of AdaNet for production machine learning systems.",url:"https://arxiv.org/abs/1905.00080"},{title:"Agnostic Learning with Multiple Objectives",description:"Multi-objective optimization framework for agnostic learning scenarios.",url:"https://proceedings.neurips.cc//paper/2020/hash/ebea2325dc670423afe9a1f4d9d1aef5-Abstract.html"}]},a={title:"LLMs",summary:"Research on efficient training methods and understanding the theoretical foundations of large language models and transformers.",papers:[{title:"Deep Fusion: Efficient Network Training via Pre-trained Initializations",description:"Novel approach to efficiently train deep networks by fusing pre-trained model initializations.",url:"https://proceedings.mlr.press/v235/mazzawi24a.html"},{title:"Learning without training: The implicit dynamics of in-context learning",description:"Theoretical insights into how transformers learn in-context without explicit parameter updates.",url:"https://arxiv.org/abs/2507.16003"},{title:"Transmuting prompts into weights",description:"Understanding the relationship between prompts and model parameters in modern transformers.",url:"https://arxiv.org/abs/2510.08734"}]};return n.createElement("div",{class:"sm:px-8 mt-24 md:mt-28"},n.createElement("div",{class:"mx-auto max-w-7xl lg:px-8"},n.createElement("div",{class:"relative px-4 sm:px-8 lg:px-12"},n.createElement("div",{class:"mx-auto max-w-2xl lg:max-w-5xl"},n.createElement("header",{class:"max-w-2xl mb-12"},n.createElement("h2",{class:"text-4xl font-bold tracking-tight text-zinc-800 dark:text-zinc-100 sm:text-5xl"},"Research projects"),n.createElement("p",{class:"mt-6 text-base text-zinc-600 dark:text-zinc-400"},"My research spans across speech synthesis, automated machine learning, and large language models.")),n.createElement("div",{class:"grid grid-cols-1 gap-8 lg:grid-cols-3"},n.createElement(l,e),n.createElement(l,t),n.createElement(l,a))))))};var c=function(){return n.createElement(i.Z,null,n.createElement("div",{class:"relative"},n.createElement(s.Z,null),n.createElement("div",{style:{height:"var(--content-offset);"}}),n.createElement("main",null,n.createElement(o,null)),n.createElement(r.Z,null)))}}}]);
//# sourceMappingURL=component---src-pages-projects-js-1e48c025189a9217fe86.js.map